{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ffdde0",
   "metadata": {},
   "source": [
    "## Anomaly detection with parametrized quantum circuits\n",
    "\n",
    "With this example we want to study the quantum version of a classic machine learning algorithm known as anomaly detection.\n",
    "\n",
    "### Background\n",
    "\n",
    "In classical machine learning, anomaly detection is implemented with an artificial neural network architecture called autoencoder. In quantum machine learning, the autoencoder is realised using parametrized quantum circuits.The proposed algorithm is not meant to outperform the classical counterpart on classical data. This example aims to demonstrate that it is possible to use quantum variational algorithms for anomaly detection with possible future advantages in the analysis of quantum data. In the next two sections a short introductions about anomaly detection algorithms and paametrized quantum circuits are reported.\n",
    "\n",
    "#### Anomaly detection\n",
    "\n",
    "Anomaly detection is a classification algorithm that allows to identify anomalous data. The advantage in using this machine learning technique is that only standard data are required for the training.\n",
    "To achieve this it's necessary to train a particular artificial neural network (ANN) architecture called autoencoder. An autoencoder is composed of two main parts: encoder and decoder.\n",
    "The encoder compresses initial data down to a small dimension (called latent dimension). The decoder inverts the process to reconstruct the original data from the compressed one. The parameters of the neural network are trained in order to minimise the difference between the initial and reconstructed data. The loss function (also called reconstruction loss) is therefore a measure of how accurately the reconstructed data resembles the original.\n",
    "\n",
    "For anomaly detection, the autoencoder is trained only on data samples belonging to the standard class. When the trained model is applied to new samples we expect the loss function to have different values for standard and anomalous data.\n",
    "By choosing a threshold value for the loss function it is possible to classify an input based on whether its reconstruction loss lands above or below this threshold. The ROC curve (Receiver operating characteristic) indicates the true positive rate and false positive rate as a function of the threshold. This can help to set the threshold value in order to maximize true positive classifications and minimize false positives.\n",
    "\n",
    "#### Parametrized quantum circuits\n",
    "\n",
    "A parametrized quantum circuit (PQC), also known as variational quantum circuits, can be used as the quantum counterpart of classical ANNs. In this kind of circuits the input information is stored in the initial state of the qubits. It can be stored as the phase (phase encoding) or in the states amplitudes (amplitude encoding). The initial state is transformed using rotation gates and entangling gates, usually controlled-not (C-NOT) gates. These gates can be organised in layers, in this circuit architecture one layer is composed of rotation gates (R_x, R_y, R_z) acting on all qubits followed by a series of C-NOT gates coupling neighbouring qubits. The trainable weights are the angles of rotation gates and can be trained using standard backpropagation (implemented with Tensorflow).\n",
    "\n",
    "A quantum circuit implements a unitary, thus invertible, transformation on the initial state. This represents a great advantage for the autoencoder architecture, as the decoder can be taken as the inverse of the encoder quantum circuit. In order to compress information the encoder circuit has to disentangle and set to zero state a given number of qubits. The loss function is thus taken as the expected measurement values of these qubits. In this way, for the training of the circuit, it is necessary only the encoder.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this example anomaly detection is performed on handwritten digits of the MNIST dataset using zeros as the standard data and ones as the anomalous data.\n",
    "\n",
    "Loading required features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ffa9e",
   "metadata": {},
   "source": [
    "Load [MNIST dataset](https://keras.io/api/datasets/mnist/) and merge test and train sets (we will make this division later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train, labels_train), (data_test, labels_test) = mnist.load_data()\n",
    "data = tf.concat([data_train, data_test], 0)\n",
    "labels = tf.concat([labels_train, labels_test], 0)\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be8bcf",
   "metadata": {},
   "source": [
    "Divide between standandard data (zero digits) and anomalous data (one digits). After that reshape the images to 8x8 pixels and then convert them to vectors. Note that it is necessary to add one channel to reshape images with tensorflow. At last normalize images, this is necessary for amplitude encoding. In order to normalise images we have to convert them to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f203734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide standard and anomalous data\n",
    "standard_data = data[labels==0]\n",
    "anomalous_data = data[labels==1]\n",
    "\n",
    "# Reshape images\n",
    "standard_data = tf.reshape(standard_data, [-1,28,28,1])\n",
    "standard_data = tf.image.resize(standard_data, [8,8])\n",
    "standard_data = tf.reshape(standard_data, [-1,64])\n",
    "\n",
    "anomalous_data = tf.reshape(anomalous_data, [-1,28,28,1])\n",
    "anomalous_data = tf.image.resize(anomalous_data, [8,8])\n",
    "anomalous_data = tf.reshape(anomalous_data, [-1,64])\n",
    "\n",
    "# Normalise images\n",
    "standard_data_np=standard_data.numpy()\n",
    "anomalous_data_np=anomalous_data.numpy()\n",
    "    \n",
    "for i in range(len(standard_data)):\n",
    "    standard_data_np[i]=standard_data_np[i]/np.linalg.norm(standard_data_np[i])\n",
    "    \n",
    "for i in range(len(anomalous_data)):\n",
    "    anomalous_data_np[i]=anomalous_data_np[i]/np.linalg.norm(anomalous_data_np[i])\n",
    "    \n",
    "standard_data=tf.convert_to_tensor(standard_data_np)\n",
    "anomalous_data=tf.convert_to_tensor(anomalous_data_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c023c",
   "metadata": {},
   "source": [
    "Now we have 6903 standard data and 7877 anomalous data in the form of vectors of 64 elements. Let us visualize two of them as 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard data shape:\", standard_data.shape)\n",
    "print(\"Anomalous data shape:\", anomalous_data.shape)\n",
    "\n",
    "print(\"STANDARD\")\n",
    "plt.imshow(tf.reshape(standard_data[4], [8,8]), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ANOMALOUS\")\n",
    "plt.imshow(tf.reshape(anomalous_data[1], [8,8]), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96d596",
   "metadata": {},
   "source": [
    "### Quantum circuit definition\n",
    "\n",
    "The dataset is ready, now we can create the circuit ansatz. We will do it with Qibo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea730848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qibo\n",
    "from qibo import gates\n",
    "from qibo.models import Circuit\n",
    "\n",
    "qibo.set_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e268f",
   "metadata": {},
   "source": [
    "#### Hyper-parameters of the circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a855a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of qubits in the circuit (6 qubits for 64 features)\n",
    "n_qubits=6\n",
    "# Number of circuit layers\n",
    "n_layers=6\n",
    "# Number of qubits to be compressed\n",
    "q_compression=3\n",
    "\n",
    "# Number of trainable parameters, defined by the other hyper-parameters\n",
    "n_params=(n_layers*n_qubits+q_compression)*3\n",
    "\n",
    "print(\"Trainable parameters:\", n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210fc655",
   "metadata": {},
   "source": [
    "Create and Draw the circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder(n_qubits, n_layers, params, q_compression):\n",
    "    index=0\n",
    "    encoder = Circuit(n_qubits)\n",
    "    for i in range(n_layers):\n",
    "        for j in range(n_qubits):\n",
    "            encoder.add(gates.RX(j, params[index]))\n",
    "            encoder.add(gates.RY(j, params[index+1]))\n",
    "            encoder.add(gates.RZ(j, params[index+2]))\n",
    "            index+=3\n",
    "        \n",
    "        for j in range(n_qubits):\n",
    "            encoder.add(gates.CNOT(j,(j+1)%n_qubits))\n",
    "            \n",
    "    for j in range(q_compression):\n",
    "        encoder.add(gates.RX(j, params[index]))\n",
    "        encoder.add(gates.RY(j, params[index+1]))\n",
    "        encoder.add(gates.RZ(j, params[index+2])) \n",
    "        index+=3  \n",
    "    return encoder\n",
    "\n",
    "# Parameters are initialised with random values\n",
    "params = tf.Variable(tf.random.normal((n_params,)))\n",
    "\n",
    "encoder=make_encoder(n_qubits, n_layers, params, q_compression)\n",
    "print(\"Circuit model summary\")\n",
    "print(encoder.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675ee35",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we are ready to train the circuit. But first import the last required features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69465e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f849f",
   "metadata": {},
   "source": [
    "#### Hyper-parameters for training\n",
    "\n",
    "Learning rate decay helps avoiding barren plateaus. For more informations about this learning rate schedule see: [Tensorflow PiecewiseConstantDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PiecewiseConstantDecay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba196d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "nepochs = 14\n",
    "# Batch size\n",
    "batch_size=20\n",
    "# Dimension of the training set (the remaining samples will be used for testing)\n",
    "train_size=4000\n",
    "# File where trained parameters are saved\n",
    "filename=\"trained_params\"\n",
    "\n",
    "steps_for_epoch=math.ceil(train_size/batch_size)\n",
    "train_set=standard_data[0:train_size]\n",
    "\n",
    "# Learning rate\n",
    "boundaries = [\n",
    "    steps_for_epoch*2, \n",
    "    steps_for_epoch*4, \n",
    "    steps_for_epoch*6, \n",
    "    steps_for_epoch*8, \n",
    "    steps_for_epoch*10, \n",
    "    steps_for_epoch*12\n",
    "]\n",
    "values = [0.4, 0.2, 0.08, 0.04, 0.01, 0.005, 0.001]\n",
    "learning_rate_fn = schedules.PiecewiseConstantDecay(\n",
    "    boundaries, values)\n",
    "optimizer = Adam(learning_rate=learning_rate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c40c6",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "Let us define the loss function and a training step.\n",
    "The loss is the sum of the ground state measurement probabilities of the compressed qubits. Thus this loss forces the compressed qubits in the |1> state.\n",
    "At each training step the loss is computed on a batch and parameters are updated with backpropagation.\n",
    "\n",
    "NB: For a training speed-up it is possible to use @tf.function decorators. However this can generate some problems for model evaluation (common error: \"Operation object has no attribute _Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def compute_loss(encoder, params, vector):\n",
    "    encoder.set_parameters(params)\n",
    "    out=encoder(vector)\n",
    "    # 3 compressed qubits\n",
    "    loss=out.probabilities(qubits=[0])[0] + out.probabilities(qubits=[1])[0] + out.probabilities(qubits=[2])[0]\n",
    "    return loss\n",
    "\n",
    "# @tf.function\n",
    "def train_step(batch_size, encoder, params, dataset):\n",
    "    loss=0.\n",
    "    with tf.GradientTape() as tape:\n",
    "        for sample in range(batch_size):\n",
    "            loss=loss+compute_loss(encoder, params, dataset[sample])\n",
    "        loss=loss/batch_size\n",
    "        grads = tape.gradient(loss, params)\n",
    "        optimizer.apply_gradients(zip([grads], [params]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7af102",
   "metadata": {},
   "source": [
    "Now we can start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This array contains the trained parameters at each epoch\n",
    "trained_params = np.zeros((nepochs, n_params), dtype=float)\n",
    "\n",
    "print(\"Start training\")\n",
    "for epoch in range(nepochs):\n",
    "    tf.random.shuffle(train_set)\n",
    "    for i in range(steps_for_epoch):\n",
    "        loss=train_step(batch_size, encoder, params, train_set[i*batch_size: (i+1)*batch_size])\n",
    "    trained_params[epoch]=params.numpy()\n",
    "    print(\"Epoch: %d  Loss: %f\" % (epoch+1,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62436512",
   "metadata": {},
   "source": [
    "If you are running the script locally you can save the trained parameters of the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose epoch parameters you want to use\n",
    "best_epoch=nepochs\n",
    "# np.save(filename, trained_params[best_epoch-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b0861",
   "metadata": {},
   "source": [
    "### Performance test\n",
    "\n",
    "Test the performance of the model on the test set.\n",
    "First create the two test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_test_set = standard_data[train_size:]\n",
    "dim_test = len(standard_test_set)\n",
    "anomalous_test_set = anomalous_data[0:dim_test]\n",
    "\n",
    "print(\"Test will be performed on %d standard and %d anomalous samples.\" % (dim_test, dim_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85423f07",
   "metadata": {},
   "source": [
    "Compute the loss on test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.set_parameters(tf.convert_to_tensor(trained_params[best_epoch-1]))\n",
    "encoder.compile()\n",
    "\n",
    "def compute_loss_test(encoder, vector):\n",
    "    out=encoder(vector)\n",
    "    loss=out.probabilities(qubits=[0])[0] + out.probabilities(qubits=[1])[0] + out.probabilities(qubits=[2])[0]\n",
    "    return loss\n",
    "\n",
    "print(\"Computing standard losses\")\n",
    "loss_s = []\n",
    "for i in range(dim_test):\n",
    "    loss_s.append(compute_loss_test(encoder, standard_test_set[i]).numpy())\n",
    "\n",
    "print(\"Computing anomalous losses\")\n",
    "loss_a = []\n",
    "for i in range(dim_test):\n",
    "    loss_a.append(compute_loss_test(encoder, anomalous_test_set[i]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8444730",
   "metadata": {},
   "source": [
    "Plot loss function distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a754886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loss_a, bins=40, histtype=\"step\", color=\"red\", label=\"Anomalous data\")\n",
    "plt.hist(loss_s, bins=40, histtype=\"step\", color=\"blue\", label=\"Standard data\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.xlabel(\"Loss value\")\n",
    "plt.title(\"Loss function distribution (MNIST dataset)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2c17a",
   "metadata": {},
   "source": [
    "Compute and plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max1=np.amax(loss_s)\n",
    "max2=np.amax(loss_a)\n",
    "ma=max(max1,max2)\n",
    "min1=np.amin(loss_s)\n",
    "min2=np.amin(loss_a)\n",
    "mi=min(min1,min2)\n",
    "\n",
    "tot_neg=len(loss_s)\n",
    "tot_pos=len(loss_a)\n",
    "\n",
    "n_step=100.\n",
    "n_step_int=100\n",
    "step=(ma-mi)/n_step\n",
    "fpr=[]\n",
    "tpr=[]\n",
    "for i in range(n_step_int):\n",
    "  treshold=i*step+mi\n",
    "  c=0\n",
    "  for j in range(tot_neg):\n",
    "    if loss_s[j]>treshold:\n",
    "      c+=1\n",
    "  false_positive=c/float(tot_neg)\n",
    "  fpr.append(false_positive)\n",
    "  c=0\n",
    "  for j in range(tot_pos):\n",
    "    if loss_a[j]>treshold:\n",
    "      c+=1\n",
    "  true_positive=c/float(tot_pos)\n",
    "  tpr.append(true_positive)\n",
    "\n",
    "\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
